{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f125cc0-a110-4f3b-ab34-16e08dc9d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch \n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476106b6-5b09-4fdd-b9fd-cdbdb51164b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 22:54:50.974134: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-02-22 22:54:50.974147: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "from pymatgen.io.cif import CifParser\n",
    "from pymatgen.analysis.local_env import CrystalNN\n",
    "from pymatgen.core import Structure, Lattice, Site\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn import Module, MultiheadAttention, Linear\n",
    "from torch_geometric.nn import global_mean_pool, GATConv\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "\n",
    "\n",
    "DATASETS = {\n",
    "    \"Mo\": \"./data/Mo\"\n",
    "}\n",
    "\n",
    "def gvector (gvector):\n",
    "    with open(gvector, \"rb\") as binary_file:\n",
    "                bin_version = int.from_bytes(binary_file.read(4),\n",
    "                                             byteorder='little',\n",
    "                                             signed=False)\n",
    "                if bin_version != 0:\n",
    "                    print(\"Version not supported!\")\n",
    "                    exit(1)\n",
    "                # converting to int to avoid handling little/big endian\n",
    "                flags = int.from_bytes(binary_file.read(2),\n",
    "                                       byteorder='little',\n",
    "                                       signed=False)\n",
    "                n_atoms = int.from_bytes(binary_file.read(4),\n",
    "                                         byteorder='little',\n",
    "                                         signed=False)\n",
    "                g_size = int.from_bytes(binary_file.read(4),\n",
    "                                        byteorder='little',\n",
    "                                        signed=False)\n",
    "                payload = binary_file.read()\n",
    "                data = np.frombuffer(payload, dtype='<f4')\n",
    "                en = data[0]\n",
    "                gvect_size = n_atoms * g_size\n",
    "                spec_tensor = np.reshape((data[1:1+n_atoms]).astype(np.int32),\n",
    "                                     [1, n_atoms])\n",
    "                gvect_tensor = np.reshape(data[1+n_atoms:1+n_atoms+gvect_size],\n",
    "                                      [n_atoms, g_size])\n",
    "    return (gvect_tensor)\n",
    "\n",
    "\n",
    "def json_to_pmg_structure(db_name, json_file, db_type):\n",
    "    \"\"\"\n",
    "    converts json files into cif format files\n",
    "    \"\"\"\n",
    "    cif_path = os.path.join(DATASETS[db_name], \n",
    "                            db_type, \"cifs\")  \n",
    "    \n",
    "    json_path = os.path.join(DATASETS[db_name], \n",
    "                            db_type, \"jsons\", json_file) \n",
    "    \n",
    "    Path(cif_path).mkdir(parents=True,\n",
    "                          exist_ok=True)\n",
    "    \n",
    "    json_data = read_json(json_path)\n",
    "    lattice_vectors = json_data[\"lattice_vectors\"]\n",
    "    lattice = Lattice(lattice_vectors)\n",
    "    sites = [\n",
    "        Site(species=atom[1], coords=atom[2], properties={\"occupancy\": 1.0})\n",
    "        for atom in json_data[\"atoms\"]\n",
    "    ]\n",
    "    cif_name = json_file.split(\".\")[0] + \".cif\"\n",
    "    structure = Structure(lattice=lattice, species=[\"Mo\"] * len(sites), coords=[site.coords for site in sites])\n",
    "    if os.path.isfile(cif_path + \"/\" + cif_name):\n",
    "        pass\n",
    "    else:\n",
    "        structure.to(filename=cif_path + \"/\" + cif_name)\n",
    "    return structure\n",
    "\n",
    "\n",
    "def get_edge_indexes(structure):\n",
    "    bonded_structure = CrystalNN(weighted_cn=True, distance_cutoffs=(10,  20.))\n",
    "    bonded_structure = bonded_structure.get_bonded_structure(structure)\n",
    "    bonded_structure = bonded_structure.as_dict()\n",
    "    structure_graph = bonded_structure[\"graphs\"][\"adjacency\"]\n",
    "\n",
    "    # len(graph) = number of atoms\n",
    "    edge_index_from = []\n",
    "    edge_index_to = []\n",
    "    edges = []\n",
    "    for i in range (len(structure_graph)):\n",
    "        #iterates over the connected atoms of each atom in the cell\n",
    "        for j in range(len(structure_graph[i])):\n",
    "            edge_index_from.append(i)\n",
    "            edge_id = structure_graph[i][j][\"id\"]\n",
    "            edge_index_to.append(edge_id)\n",
    "            edge = torch.tensor(structure_graph[i][j][\"to_jimage\"])\n",
    "            edges.append(edge)\n",
    "\n",
    "    edge_index_from = torch.tensor(edge_index_from)\n",
    "    edge_index_to = torch.tensor(edge_index_to)\n",
    "\n",
    "    edge_indexes = np.array([edge_index_from, edge_index_to])\n",
    "    edge_indexes = torch.from_numpy(edge_indexes)\n",
    "\n",
    "    edges = np.array(edges)\n",
    "    edges = torch.from_numpy(edges)\n",
    "    return edge_indexes, edges\n",
    "\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_db_keys(db_name, db_type):\n",
    "    db_path = os.path.join(DATASETS[db_name], db_type, \"gvectors\")\n",
    "    keys = [f.split(\".\")[0] for f in os.listdir(db_path) if os.path.isfile(os.path.join(db_path, f))]\n",
    "\n",
    "    gvector_keys = []\n",
    "    json_keys = []\n",
    "    for item in keys:\n",
    "        gvector_keys.append(item+\".bin\")\n",
    "        json_keys.append(item+\".example\")\n",
    "                  \n",
    "    return gvector_keys, json_keys\n",
    "\n",
    "\n",
    "\n",
    "def dataset(db_name, db_type):\n",
    "    # Parinello vectors\n",
    "    db  = db_type\n",
    "    db_path =  os.path.join(DATASETS[db_name], db_type, \"gvectors\")\n",
    "    gvect_keys, json_keys = get_db_keys(db_name, db_type=db)\n",
    "    set = []\n",
    "    for item in gvect_keys[:]:\n",
    "        a = gvector (db_path + \"/\" + item)\n",
    "        a = torch.tensor(a)\n",
    "        set.append(a)\n",
    "    parinello = set\n",
    "\n",
    "    # edge indexes\n",
    "    edge_indexes = []\n",
    "    edges = []\n",
    "\n",
    "    for item in tqdm(json_keys[:]):\n",
    "        structure = json_to_pmg_structure(db_name=\"Mo\", json_file=item, db_type=db)\n",
    "        ei, e = get_edge_indexes(structure)\n",
    "        edge_indexes.append(ei)\n",
    "        edges.append(e)\n",
    "         \n",
    "    return parinello, edge_indexes, edges\n",
    "\n",
    "\n",
    "def get_labels(db_name, db_type):\n",
    "     \"\"\"gets labels (energy, force, ...)\"\"\"\n",
    "\n",
    "     db = db_type\n",
    "     label = []\n",
    "     db_path =  os.path.join(DATASETS[db_name], db_type, \"jsons\")\n",
    "     gvect_keys, json_keys = get_db_keys(db_name, db_type=db)\n",
    "     \n",
    "     for item in json_keys[:]:\n",
    "          example = os.path.join(db_path, item)\n",
    "          data = read_json(example)\n",
    "          num_atoms = len(data[\"atoms\"])\n",
    "          toten = data[\"energy\"][0]\n",
    "          en_per_atom = toten/num_atoms\n",
    "          label.append(en_per_atom)\n",
    "     \n",
    "     label = torch.tensor(label, dtype=torch.float)\n",
    "     \n",
    "     return label\n",
    "\n",
    "def create_sequence_tensor(feature, seq_len):\n",
    "    count = 0\n",
    "    sequence = []\n",
    "    num_batches = len(feature) // seq_len\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        sub_sequence = [feature[count + i] for i in range(seq_len)]\n",
    "        count += seq_len\n",
    "        sequence.append(sub_sequence)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def in_context_data(data_loader, batch_size):\n",
    "    in_context_db = []\n",
    "    for batch in data_loader:\n",
    "        in_context_example = {\n",
    "            \"parinello\": batch.x,\n",
    "            \"edge_index\": batch.edge_index,\n",
    "            \"to_j\": batch.to_j,\n",
    "            \"in_context_label\": batch.batch,\n",
    "            \"label\": batch.y, \n",
    "        }\n",
    "\n",
    "        data = Data(x=in_context_example[\"parinello\"], edge_index=in_context_example[\"edge_index\"],\n",
    "            to_j=in_context_example[\"to_j\"], config_label=in_context_example[\"in_context_label\"],\n",
    "            y=in_context_example[\"label\"])\n",
    "    \n",
    "        in_context_db.append(data)\n",
    "\n",
    "    context_loader = DataLoader(in_context_db, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return context_loader\n",
    "\n",
    "\n",
    "def data(db_name, sequence_size, batch_size, db_type):\n",
    "    \"\"\"Create a PyTorch Geometric Data object\"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    db = db_type\n",
    "    parinello, edge_indexes, edges = dataset(db_name=db_name, db_type=db)\n",
    "    labels = get_labels(db_name, db_type=db)\n",
    "\n",
    "    db = []\n",
    "    for i in range (len(parinello)):\n",
    "        data = Data(x=parinello[i], edge_index=edge_indexes[i], to_j=edges[i], y=labels[i])\n",
    "        db.append(data)\n",
    "\n",
    "    # Create a PyTorch Geometric DataLoader\n",
    "    batch_size = batch_size\n",
    "\n",
    "    t_loader = DataLoader(db, batch_size=sequence_size, shuffle=False)\n",
    "    loader = in_context_data(t_loader, batch_size=batch_size)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1878d4cd-a6b6-4596-96b8-caedfb179d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/Mo/train/gvectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 232\u001b[0m, in \u001b[0;36mdata\u001b[0;34m(db_name, sequence_size, batch_size, db_type)\u001b[0m\n\u001b[1;32m    230\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m db \u001b[38;5;241m=\u001b[39m db_type\n\u001b[0;32m--> 232\u001b[0m parinello, edge_indexes, edges \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m labels \u001b[38;5;241m=\u001b[39m get_labels(db_name, db_type\u001b[38;5;241m=\u001b[39mdb)\n\u001b[1;32m    235\u001b[0m db \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[5], line 153\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(db_name, db_type)\u001b[0m\n\u001b[1;32m    151\u001b[0m db  \u001b[38;5;241m=\u001b[39m db_type\n\u001b[1;32m    152\u001b[0m db_path \u001b[38;5;241m=\u001b[39m  os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASETS[db_name], db_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgvectors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m gvect_keys, json_keys \u001b[38;5;241m=\u001b[39m \u001b[43mget_db_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mset\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m gvect_keys[:]:\n",
      "Cell \u001b[0;32mIn[5], line 137\u001b[0m, in \u001b[0;36mget_db_keys\u001b[0;34m(db_name, db_type)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_db_keys\u001b[39m(db_name, db_type):\n\u001b[1;32m    136\u001b[0m     db_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASETS[db_name], db_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgvectors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m     keys \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(db_path, f))]\n\u001b[1;32m    139\u001b[0m     gvector_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    140\u001b[0m     json_keys \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/Mo/train/gvectors'"
     ]
    }
   ],
   "source": [
    "loader = data(db_name=\"Mo\", sequence_size=4 , batch_size=4, db_type = \"train\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
