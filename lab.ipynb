{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "from pymatgen.io.cif import CifParser\n",
    "from pymatgen.analysis.local_env import CrystalNN\n",
    "from pymatgen.core import Structure, Lattice, Site\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn import Module, MultiheadAttention, Linear\n",
    "from torch_geometric.nn import global_mean_pool, GATConv\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "\n",
    "DATASETS = {\n",
    "    \"Mo\": \"./data/Mo\"\n",
    "}\n",
    "\n",
    "def gvector (gvector):\n",
    "    with open(gvector, \"rb\") as binary_file:\n",
    "                bin_version = int.from_bytes(binary_file.read(4),\n",
    "                                             byteorder='little',\n",
    "                                             signed=False)\n",
    "                if bin_version != 0:\n",
    "                    print(\"Version not supported!\")\n",
    "                    exit(1)\n",
    "                # converting to int to avoid handling little/big endian\n",
    "                flags = int.from_bytes(binary_file.read(2),\n",
    "                                       byteorder='little',\n",
    "                                       signed=False)\n",
    "                n_atoms = int.from_bytes(binary_file.read(4),\n",
    "                                         byteorder='little',\n",
    "                                         signed=False)\n",
    "                g_size = int.from_bytes(binary_file.read(4),\n",
    "                                        byteorder='little',\n",
    "                                        signed=False)\n",
    "                payload = binary_file.read()\n",
    "                data = np.frombuffer(payload, dtype='<f4')\n",
    "                en = data[0]\n",
    "                gvect_size = n_atoms * g_size\n",
    "                spec_tensor = np.reshape((data[1:1+n_atoms]).astype(np.int32),\n",
    "                                     [1, n_atoms])\n",
    "                gvect_tensor = np.reshape(data[1+n_atoms:1+n_atoms+gvect_size],\n",
    "                                      [n_atoms, g_size])\n",
    "    return (gvect_tensor)\n",
    "\n",
    "\n",
    "def json_to_pmg_structure(db_name, json_file):\n",
    "    \"\"\"\n",
    "    converts json files into cif format files\n",
    "    \"\"\"\n",
    "    cif_path = os.path.join(DATASETS[db_name], \n",
    "                            \"train_gv\", \"cifs\")  \n",
    "    \n",
    "    json_path = os.path.join(DATASETS[db_name], \n",
    "                            \"train_gv\", \"jsons\", json_file) \n",
    "    \n",
    "    Path(cif_path).mkdir(parents=True,\n",
    "                          exist_ok=True)\n",
    "    \n",
    "    json_data = read_json(json_path)\n",
    "    lattice_vectors = json_data[\"lattice_vectors\"]\n",
    "    lattice = Lattice(lattice_vectors)\n",
    "    sites = [\n",
    "        Site(species=atom[1], coords=atom[2], properties={\"occupancy\": 1.0})\n",
    "        for atom in json_data[\"atoms\"]\n",
    "    ]\n",
    "    cif_name = json_file.split(\".\")[0] + \".cif\"\n",
    "    structure = Structure(lattice=lattice, species=[\"Mo\"] * len(sites), coords=[site.coords for site in sites])\n",
    "    if os.path.isfile(cif_path + \"/\" + cif_name):\n",
    "        pass\n",
    "    else:\n",
    "        structure.to(filename=cif_path + \"/\" + cif_name)\n",
    "    return structure\n",
    "\n",
    "\n",
    "def get_edge_indexes(structure):\n",
    "    bonded_structure = CrystalNN(weighted_cn=True, distance_cutoffs=(10,  20.))\n",
    "    bonded_structure = bonded_structure.get_bonded_structure(structure)\n",
    "    bonded_structure = bonded_structure.as_dict()\n",
    "    structure_graph = bonded_structure[\"graphs\"][\"adjacency\"]\n",
    "\n",
    "    # len(graph) = number of atoms\n",
    "    edge_index_from = []\n",
    "    edge_index_to = []\n",
    "    edges = []\n",
    "    for i in range (len(structure_graph)):\n",
    "        #iterates over the connected atoms of each atom in the cell\n",
    "        for j in range(len(structure_graph[i])):\n",
    "            edge_index_from.append(i)\n",
    "            edge_id = structure_graph[i][j][\"id\"]\n",
    "            edge_index_to.append(edge_id)\n",
    "            edge = torch.tensor(structure_graph[i][j][\"to_jimage\"])\n",
    "            edges.append(edge)\n",
    "\n",
    "    edge_index_from = torch.tensor(edge_index_from)\n",
    "    edge_index_to = torch.tensor(edge_index_to)\n",
    "\n",
    "    edge_indexes = np.array([edge_index_from, edge_index_to])\n",
    "    edge_indexes = torch.from_numpy(edge_indexes)\n",
    "\n",
    "    edges = np.array(edges)\n",
    "    edges = torch.from_numpy(edges)\n",
    "    return edge_indexes, edges\n",
    "\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_db_keys(db_name):\n",
    "    db_path = os.path.join(DATASETS[db_name], \"train_gv\", \"gvectors\")\n",
    "    keys = [f.split(\".\")[0] for f in os.listdir(db_path) if os.path.isfile(os.path.join(db_path, f))]\n",
    "\n",
    "    gvector_keys = []\n",
    "    json_keys = []\n",
    "    for item in keys:\n",
    "        gvector_keys.append(item+\".bin\")\n",
    "        json_keys.append(item+\".example\")\n",
    "                  \n",
    "    return gvector_keys, json_keys\n",
    "\n",
    "\n",
    "\n",
    "def dataset(db_name):\n",
    "    # Parinello vectors\n",
    "    db_path =  os.path.join(DATASETS[db_name], \"train_gv\", \"gvectors\")\n",
    "    gvect_keys, json_keys = get_db_keys(db_name)\n",
    "    set = []\n",
    "    for item in gvect_keys[0:50]:\n",
    "        a = gvector (db_path + \"/\" + item)\n",
    "        a = torch.tensor(a)\n",
    "        set.append(a)\n",
    "    parinello = set\n",
    "\n",
    "    # edge indexes\n",
    "    edge_indexes = []\n",
    "    edges = []\n",
    "\n",
    "    for item in tqdm(json_keys[0:50]):\n",
    "        structure = json_to_pmg_structure(db_name=\"Mo\", json_file=item)\n",
    "        ei, e = get_edge_indexes(structure)\n",
    "        edge_indexes.append(ei)\n",
    "        edges.append(e)\n",
    "         \n",
    "    return parinello, edge_indexes, edges\n",
    "\n",
    "\n",
    "def get_labels(db_name):\n",
    "     \"\"\"gets labels (energy, force, ...)\"\"\"\n",
    "     \n",
    "     label = []\n",
    "     db_path =  os.path.join(DATASETS[db_name], \"train_gv\", \"jsons\")\n",
    "     gvect_keys, json_keys = get_db_keys(db_name)\n",
    "     \n",
    "     for item in json_keys[0: 50]:\n",
    "          example = os.path.join(db_path, item)\n",
    "          data = read_json(example)\n",
    "          num_atoms = len(data[\"atoms\"])\n",
    "          toten = data[\"energy\"][0]\n",
    "          en_per_atom = toten/num_atoms\n",
    "          label.append(en_per_atom)\n",
    "     \n",
    "     label = torch.tensor(label, dtype=torch.float)\n",
    "     \n",
    "     return label\n",
    "\n",
    "def create_sequence_tensor(feature, seq_len):\n",
    "    count = 0\n",
    "    sequence = []\n",
    "    num_batches = len(feature) // seq_len\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        sub_sequence = [feature[count + i] for i in range(seq_len)]\n",
    "        count += seq_len\n",
    "        sequence.append(sub_sequence)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def in_context_data(data_loader, batch_size):\n",
    "    in_context_db = []\n",
    "    for batch in data_loader:\n",
    "        in_context_example = {\n",
    "            \"parinello\": batch.x,\n",
    "            \"edge_index\": batch.edge_index,\n",
    "            \"to_j\": batch.to_j,\n",
    "            \"in_context_label\": batch.batch,\n",
    "            \"label\": batch.y, \n",
    "        }\n",
    "\n",
    "        data = Data(x=in_context_example[\"parinello\"], edge_index=in_context_example[\"edge_index\"],\n",
    "            to_j=in_context_example[\"to_j\"], config_label=in_context_example[\"in_context_label\"],\n",
    "            y=in_context_example[\"label\"])\n",
    "    \n",
    "        in_context_db.append(data)\n",
    "\n",
    "    context_loader = DataLoader(in_context_db, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # BIG ? here\n",
    "    for batch in context_loader:\n",
    "        graphs_per_datapoint = torch.max(batch.config_label) + 1\n",
    "        actual_batch_dot_product = batch.batch * graphs_per_datapoint + batch.config_label\n",
    "\n",
    "    return context_loader\n",
    "\n",
    "\n",
    "def data(db_name, sequence_size, batch_size):\n",
    "    \"\"\"Create a PyTorch Geometric Data object\"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    parinello, edge_indexes, edges = dataset(db_name=db_name)\n",
    "    labels = get_labels(db_name)\n",
    "\n",
    "    db = []\n",
    "    for i in range (len(parinello)):\n",
    "        data = Data(x=parinello[i], edge_index=edge_indexes[i], to_j=edges[i], y=labels[i])\n",
    "        db.append(data)\n",
    "\n",
    "    # Create a PyTorch Geometric DataLoader\n",
    "    batch_size = batch_size\n",
    "    dataset_size = len(db)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    val_size = dataset_size - train_size\n",
    "    train_dataset, val_dataset = random_split(db, [train_size, val_size])\n",
    "\n",
    "    t_loader = DataLoader(train_dataset, batch_size=sequence_size, shuffle=False)\n",
    "    v_loader = DataLoader(val_dataset, batch_size=sequence_size, shuffle=False)\n",
    "    \n",
    "    train_loader = in_context_data(t_loader, batch_size=batch_size)\n",
    "    val_loader = in_context_data(v_loader, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#MODEL\n",
    "\n",
    "class AtteMtion(Module):\n",
    "    def __init__(self, in_channels, out_channels, heads):\n",
    "        super(AtteMtion, self).__init__()\n",
    "        self.lin_k = Linear(in_channels, out_channels)\n",
    "        self.lin_q = Linear(in_channels, out_channels)\n",
    "        self.lin_v = Linear(in_channels, out_channels)\n",
    "        self.att = MultiheadAttention(out_channels, heads, batch_first=True)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        K = self.lin_k(h)\n",
    "        Q = self.lin_q(h)\n",
    "        V = self.lin_v(h)\n",
    "        out = self.att(K[:, None, :], Q[:, None, :], V[:, None, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class InContextGNN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(InContextGNN, self).__init__()\n",
    "        self.graph1 = GATConv(in_channels=160, out_channels=16, heads=2)\n",
    "        self.graph2 = GATConv(in_channels=32, out_channels=8, heads=8)\n",
    "        self.att1 = AtteMtion(64, 8, 2)\n",
    "        self.readout = Linear(8, 1)\n",
    "        self.act = relu\n",
    "        self.train_loader, self.val_loader = data(\"Mo\", 10) \n",
    " \n",
    "    def forward(self, batch):\n",
    "        #encoder\n",
    "        graphs_per_datapoint = torch.max(batch.config_label) + 1\n",
    "        actual_batch_dot_batch = batch.batch * graphs_per_datapoint + batch.config_label\n",
    "\n",
    "        graph_h1 = self.graph1(batch.x, batch.edge_index)\n",
    "        graph_h1 = self.act(graph_h1)\n",
    "        graph_h2 = self.graph2(graph_h1, batch.edge_index)\n",
    "        graph_h2 = self.act(graph_h2)\n",
    "        graph_h = global_mean_pool(graph_h2, actual_batch_dot_batch)\n",
    "        batch.config_label\n",
    "        h1 = self.att1(graph_h)\n",
    "        h1 = self.act(h1[0])\n",
    "        out = self.readout(h1[0:])\n",
    "        return out\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = self.train_loader\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = self.val_loader\n",
    "        return val_loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch.y.view(-1, 1))\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('learning_rate', self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=0.01)\n",
    "        scheduler = StepLR(optimizer, step_size=10000, gamma=0.1)\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch', \n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch.y.view(-1, 1))\n",
    "        self.log('val_loss', loss)\n",
    "        return {'val_loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:34<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = data(\"Mo\",4 , 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = next(iter(train_loader))\n",
    "d.config_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "               batch.to(device) # if needed\n",
    "               # assuming the number of in-context examples is constant, we can:\n",
    "\n",
    "               graphs_per_datapoint = torch.max(batch.config_label) + 1 # it’s 5 for us, the number of graphs in a sequence\n",
    "\n",
    "               # I’ll add a silly name to make it comprehensive 😉\n",
    "\n",
    "            actual_batch_dot_batch = batch.batch*graphs_per_datapoint + batch.config_label\n",
    "              \n",
    "\n",
    "(…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, batch):\n",
    "    \n",
    "    graphs_per_datapoint = torch.max(batch.config_label) + 1\n",
    "    actual_batch_dot_product = batch.batch * graphs_per_datapoint + batch.config_label\n",
    "\n",
    "    graph_h1 = self.graph1(batch.x, batch.edge_index)\n",
    "    graph_h1 = self.act(graph_h1)\n",
    "    graph_h2 = self.graph2(graph_h1, batch.edge_index)\n",
    "    graph_h2 = self.act(graph_h2)\n",
    "    graph_h = global_mean_pool(graph_h2, batch.batch)\n",
    "    graph_h = self.act(graph_h)\n",
    "    h1 = self.att1(graph_h)\n",
    "    h1 = self.act(h1[0])\n",
    "    out = self.readout(h1[0:])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "tensor([0, 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graphs_per_datapoint = torch.max(batch.config_label) + 1\n",
    "print(graphs_per_datapoint)\n",
    "actual_batch_dot_product = batch.batch * graphs_per_datapoint + batch.config_label\n",
    "print(actual_batch_dot_product)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
