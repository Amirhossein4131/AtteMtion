{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "from pymatgen.io.cif import CifParser\n",
    "from pymatgen.analysis.local_env import CrystalNN\n",
    "from pymatgen.core import Structure, Lattice, Site\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn import Module, MultiheadAttention, Linear\n",
    "from torch_geometric.nn import global_mean_pool, GATConv\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "\n",
    "\n",
    "DATASETS = {\n",
    "    \"Mo\": \"./data/Mo\"\n",
    "}\n",
    "\n",
    "def gvector (gvector):\n",
    "    with open(gvector, \"rb\") as binary_file:\n",
    "                bin_version = int.from_bytes(binary_file.read(4),\n",
    "                                             byteorder='little',\n",
    "                                             signed=False)\n",
    "                if bin_version != 0:\n",
    "                    print(\"Version not supported!\")\n",
    "                    exit(1)\n",
    "                # converting to int to avoid handling little/big endian\n",
    "                flags = int.from_bytes(binary_file.read(2),\n",
    "                                       byteorder='little',\n",
    "                                       signed=False)\n",
    "                n_atoms = int.from_bytes(binary_file.read(4),\n",
    "                                         byteorder='little',\n",
    "                                         signed=False)\n",
    "                g_size = int.from_bytes(binary_file.read(4),\n",
    "                                        byteorder='little',\n",
    "                                        signed=False)\n",
    "                payload = binary_file.read()\n",
    "                data = np.frombuffer(payload, dtype='<f4')\n",
    "                en = data[0]\n",
    "                gvect_size = n_atoms * g_size\n",
    "                spec_tensor = np.reshape((data[1:1+n_atoms]).astype(np.int32),\n",
    "                                     [1, n_atoms])\n",
    "                gvect_tensor = np.reshape(data[1+n_atoms:1+n_atoms+gvect_size],\n",
    "                                      [n_atoms, g_size])\n",
    "    return (gvect_tensor)\n",
    "\n",
    "\n",
    "def json_to_pmg_structure(db_name, json_file):\n",
    "    \"\"\"\n",
    "    converts json files into cif format files\n",
    "    \"\"\"\n",
    "    cif_path = os.path.join(DATASETS[db_name], \n",
    "                            \"train_gv\", \"cifs\")  \n",
    "    \n",
    "    json_path = os.path.join(DATASETS[db_name], \n",
    "                            \"train_gv\", \"jsons\", json_file) \n",
    "    \n",
    "    Path(cif_path).mkdir(parents=True,\n",
    "                          exist_ok=True)\n",
    "    \n",
    "    json_data = read_json(json_path)\n",
    "    lattice_vectors = json_data[\"lattice_vectors\"]\n",
    "    lattice = Lattice(lattice_vectors)\n",
    "    sites = [\n",
    "        Site(species=atom[1], coords=atom[2], properties={\"occupancy\": 1.0})\n",
    "        for atom in json_data[\"atoms\"]\n",
    "    ]\n",
    "    cif_name = json_file.split(\".\")[0] + \".cif\"\n",
    "    structure = Structure(lattice=lattice, species=[\"Mo\"] * len(sites), coords=[site.coords for site in sites])\n",
    "    if os.path.isfile(cif_path + \"/\" + cif_name):\n",
    "        pass\n",
    "    else:\n",
    "        structure.to(filename=cif_path + \"/\" + cif_name)\n",
    "    return structure\n",
    "\n",
    "\n",
    "def get_edge_indexes(structure):\n",
    "    bonded_structure = CrystalNN(weighted_cn=True, distance_cutoffs=(10,  20.))\n",
    "    bonded_structure = bonded_structure.get_bonded_structure(structure)\n",
    "    bonded_structure = bonded_structure.as_dict()\n",
    "    structure_graph = bonded_structure[\"graphs\"][\"adjacency\"]\n",
    "\n",
    "    # len(graph) = number of atoms\n",
    "    edge_index_from = []\n",
    "    edge_index_to = []\n",
    "    edges = []\n",
    "    for i in range (len(structure_graph)):\n",
    "        #iterates over the connected atoms of each atom in the cell\n",
    "        for j in range(len(structure_graph[i])):\n",
    "            edge_index_from.append(i)\n",
    "            edge_id = structure_graph[i][j][\"id\"]\n",
    "            edge_index_to.append(edge_id)\n",
    "            edge = torch.tensor(structure_graph[i][j][\"to_jimage\"])\n",
    "            edges.append(edge)\n",
    "\n",
    "    edge_index_from = torch.tensor(edge_index_from)\n",
    "    edge_index_to = torch.tensor(edge_index_to)\n",
    "\n",
    "    edge_indexes = np.array([edge_index_from, edge_index_to])\n",
    "    edge_indexes = torch.from_numpy(edge_indexes)\n",
    "\n",
    "    edges = np.array(edges)\n",
    "    edges = torch.from_numpy(edges)\n",
    "    return edge_indexes, edges\n",
    "\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_db_keys(db_name):\n",
    "    db_path = os.path.join(DATASETS[db_name], \"train_gv\", \"gvectors\")\n",
    "    keys = [f.split(\".\")[0] for f in os.listdir(db_path) if os.path.isfile(os.path.join(db_path, f))]\n",
    "\n",
    "    gvector_keys = []\n",
    "    json_keys = []\n",
    "    for item in keys:\n",
    "        gvector_keys.append(item+\".bin\")\n",
    "        json_keys.append(item+\".example\")\n",
    "                  \n",
    "    return gvector_keys, json_keys\n",
    "\n",
    "\n",
    "\n",
    "def dataset(db_name):\n",
    "    # Parinello vectors\n",
    "    db_path =  os.path.join(DATASETS[db_name], \"train_gv\", \"gvectors\")\n",
    "    gvect_keys, json_keys = get_db_keys(db_name)\n",
    "    set = []\n",
    "    for item in gvect_keys[0:50]:\n",
    "        a = gvector (db_path + \"/\" + item)\n",
    "        a = torch.tensor(a)\n",
    "        set.append(a)\n",
    "    parinello = set\n",
    "\n",
    "    # edge indexes\n",
    "    edge_indexes = []\n",
    "    edges = []\n",
    "\n",
    "    for item in tqdm(json_keys[0:50]):\n",
    "        structure = json_to_pmg_structure(db_name=\"Mo\", json_file=item)\n",
    "        ei, e = get_edge_indexes(structure)\n",
    "        edge_indexes.append(ei)\n",
    "        edges.append(e)\n",
    "         \n",
    "    return parinello, edge_indexes, edges\n",
    "\n",
    "\n",
    "def get_labels(db_name):\n",
    "     \"\"\"gets labels (energy, force, ...)\"\"\"\n",
    "     \n",
    "     label = []\n",
    "     db_path =  os.path.join(DATASETS[db_name], \"train_gv\", \"jsons\")\n",
    "     gvect_keys, json_keys = get_db_keys(db_name)\n",
    "     \n",
    "     for item in json_keys[0: 50]:\n",
    "          example = os.path.join(db_path, item)\n",
    "          data = read_json(example)\n",
    "          num_atoms = len(data[\"atoms\"])\n",
    "          toten = data[\"energy\"][0]\n",
    "          en_per_atom = toten/num_atoms\n",
    "          label.append(en_per_atom)\n",
    "     \n",
    "     label = torch.tensor(label, dtype=torch.float)\n",
    "     \n",
    "     return label\n",
    "\n",
    "def create_sequence_tensor(feature, seq_len):\n",
    "    count = 0\n",
    "    sequence = []\n",
    "    num_batches = len(feature) // seq_len\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        sub_sequence = [feature[count + i] for i in range(seq_len)]\n",
    "        count += seq_len\n",
    "        sequence.append(sub_sequence)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def in_context_data(data_loader, batch_size):\n",
    "    in_context_db = []\n",
    "    for batch in data_loader:\n",
    "        in_context_example = {\n",
    "            \"parinello\": batch.x,\n",
    "            \"edge_index\": batch.edge_index,\n",
    "            \"to_j\": batch.to_j,\n",
    "            \"in_context_label\": batch.batch,\n",
    "            \"label\": batch.y, \n",
    "        }\n",
    "\n",
    "        data = Data(x=in_context_example[\"parinello\"], edge_index=in_context_example[\"edge_index\"],\n",
    "            to_j=in_context_example[\"to_j\"], config_label=in_context_example[\"in_context_label\"],\n",
    "            y=in_context_example[\"label\"])\n",
    "    \n",
    "        in_context_db.append(data)\n",
    "\n",
    "    context_loader = DataLoader(in_context_db, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return context_loader\n",
    "\n",
    "\n",
    "def data(db_name, sequence_size, batch_size):\n",
    "    \"\"\"Create a PyTorch Geometric Data object\"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    parinello, edge_indexes, edges = dataset(db_name=db_name)\n",
    "    labels = get_labels(db_name)\n",
    "\n",
    "    db = []\n",
    "    for i in range (len(parinello)):\n",
    "        data = Data(x=parinello[i], edge_index=edge_indexes[i], to_j=edges[i], y=labels[i])\n",
    "        db.append(data)\n",
    "\n",
    "    # Create a PyTorch Geometric DataLoader\n",
    "    batch_size = batch_size\n",
    "    dataset_size = len(db)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    val_size = dataset_size - train_size\n",
    "    train_dataset, val_dataset = random_split(db, [train_size, val_size])\n",
    "\n",
    "    t_loader = DataLoader(train_dataset, batch_size=sequence_size, shuffle=False)\n",
    "    v_loader = DataLoader(val_dataset, batch_size=sequence_size, shuffle=False)\n",
    "    \n",
    "    train_loader = in_context_data(t_loader, batch_size=batch_size)\n",
    "    val_loader = in_context_data(v_loader, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#MODEL\n",
    "\n",
    "class GPT2BasedModel(Module):\n",
    "    def __init__(self, n_dims, n_positions, n_embd=128, n_layer=12, n_head=4):\n",
    "        super(GPT2BasedModel, self).__init__()\n",
    "\n",
    "        # GPT-2 Configuration\n",
    "        configuration = GPT2Config(\n",
    "            n_positions=2 * n_positions,\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            resid_pdrop=0.0,\n",
    "            embd_pdrop=0.0,\n",
    "            attn_pdrop=0.0,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        self.name = f\"gpt2_embd={n_embd}_layer={n_layer}_head={n_head}\"\n",
    "        self._read_in = Linear(n_dims, n_embd)\n",
    "        self._read_out = Linear(n_embd, 1)\n",
    "\n",
    "        self._backbone = GPT2Model(configuration)\n",
    "\n",
    "    @staticmethod\n",
    "    def _combine(xs_b, ys_b):\n",
    "        \"\"\"Interleaves the x's and the y's into a single sequence.\"\"\"\n",
    "        bsize, points, dim = xs_b.shape\n",
    "        ys_b_wide = torch.cat(\n",
    "            (\n",
    "                ys_b.view(bsize, points, 1),\n",
    "                torch.zeros(bsize, points, dim - 1, device=ys_b.device),\n",
    "            ),\n",
    "            axis=2,\n",
    "        )\n",
    "        zs = torch.stack((xs_b, ys_b_wide), dim=2)\n",
    "        zs = zs.view(bsize, 2 * points, dim)\n",
    "        return zs\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = input_tensor\n",
    "        x = self._read_in(input_tensor)\n",
    "        gpt2_output = self._backbone(inputs_embeds=x)\n",
    "        output = self._read_out(gpt2_output.last_hidden_state[:, -1, :])\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class InContextGNN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(InContextGNN, self).__init__()\n",
    "        self.graph1 = GATConv(in_channels=160, out_channels=16, heads=2)\n",
    "        self.graph2 = GATConv(in_channels=32, out_channels=8, heads=8)\n",
    "        self.att1 = GPT2BasedModel(64, 128)\n",
    "        self.readout = Linear(8, 1)\n",
    "        self.act = relu\n",
    "        self.train_loader, self.val_loader = data(\"Mo\", 10, 10) \n",
    " \n",
    "    def forward(self, batch):\n",
    "        #encoder\n",
    "        graphs_per_datapoint = torch.max(batch.config_label) + 1\n",
    "        actual_batch_dot_batch = batch.batch * graphs_per_datapoint + batch.config_label\n",
    "\n",
    "        graph_h1 = self.graph1(batch.x, batch.edge_index)\n",
    "        graph_h1 = self.act(graph_h1)\n",
    "        graph_h2 = self.graph2(graph_h1, batch.edge_index)\n",
    "        graph_h2 = self.act(graph_h2)\n",
    "        graph_h = global_mean_pool(graph_h2, actual_batch_dot_batch)\n",
    "        batch.config_label\n",
    "        h1 = self.att1(graph_h)\n",
    "        h1 = self.act(h1[0])\n",
    "        out = self.readout(h1[0:])\n",
    "        return out\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = self.train_loader\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = self.val_loader\n",
    "        return val_loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch.y.view(-1, 1))\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('learning_rate', self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=0.01)\n",
    "        scheduler = StepLR(optimizer, step_size=10000, gamma=0.1)\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch', \n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch.y.view(-1, 1))\n",
    "        self.log('val_loss', loss)\n",
    "        return {'val_loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1 = GATConv(in_channels=160, out_channels=16, heads=2)\n",
    "graph2 = GATConv(in_channels=32, out_channels=8, heads=8)\n",
    "readout = Linear(768, 1)\n",
    "readin = Linear(64, 64)\n",
    "act = relu\n",
    "gpt2_model = GPT2BasedModel(64, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:35<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = data(\"Mo\",4 , 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.6779, -10.8607, -10.8035, -10.8589, -10.9141, -10.8179, -10.5100,\n",
       "        -10.9150, -10.9002, -10.9045, -10.8406, -10.8556, -10.9251, -10.8615,\n",
       "        -10.8526,  -9.4712])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = next(iter(train_loader))\n",
    "d.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3688],\n",
       "        [-0.2766],\n",
       "        [-0.2957],\n",
       "        [-0.7259]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs_per_datapoint = torch.max(d.config_label) + 1\n",
    "actual_batch_dot_product = d.batch * graphs_per_datapoint + d.config_label\n",
    "\n",
    "graph_h1 = graph1(d.x, d.edge_index)\n",
    "graph_h1 = act(graph_h1)\n",
    "graph_h2 = graph2(graph_h1, d.edge_index)\n",
    "graph_h2 = act(graph_h2)\n",
    "graph_h = global_mean_pool(graph_h2, actual_batch_dot_product)\n",
    "\n",
    "\n",
    "graph_h = act(graph_h)\n",
    "\n",
    "#graph_h = graph_h.reshape(batch_size, examples_per_seq, -1)\n",
    "graph_h = graph_h.reshape(4, 4, -1)\n",
    "print (graph_h.shape)\n",
    "\n",
    "\n",
    "gpt_o = gpt2_model(graph_h)\n",
    "\n",
    "gpt_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
