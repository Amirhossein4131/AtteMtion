_target_: transformers.GPT2Model
config:
  _target_: transformers.GPT2Config
  n_positions: 32
  n_embd: 128
  n_layer: 12
  n_head: 4
  resid_pdrop: 0.0
  embd_pdrop: 0.0
  attn_pdrop: 0.0
  use_cache: False
