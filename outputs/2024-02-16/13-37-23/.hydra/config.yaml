run:
  hydra:
    dir: ${oc.env:PROJECT_ROOT}/hydra/regression/${now:%Y-%m-%d}/${now:%H-%M-%S}
trainer:
  max_epochs: 5000
model:
  _target_: pl_modules.modules.in_context.InContextWrap
  pool: ${load:torch_geometric.nn.global_mean_pool}
  encoder:
    _target_: pl_modules.nn.graph_models.GATWrap
    in_channels: 58
    out_channels: 128
    hidden_channels: 64
    num_layers: 3
    heads: 4
  decoder:
    _target_: transformers.GPT2Model
    config:
      _target_: transformers.GPT2Config
      n_positions: 32
      n_embd: 128
      n_layer: 12
      n_head: 4
      resid_pdrop: 0.0
      embd_pdrop: 0.0
      attn_pdrop: 0.0
      use_cache: false
  label_readout:
    _target_: torch.nn.Linear
    in_features: 128
    out_features: 1
  optimizer_cfg:
    lr: 0.1
    step_size: 1000
    gamma: 0.5
datamodule:
  _target_: pl_modules.data.datamodule.MolybdenumDataModule
  modification: no-context
  sequence_length: 2
  batch_size: 64
  db_name: EFF
  datapoint_limit: 1800
  label_scaler: null
