model:
  encoder:
    _target_: pl_modules.nn.graph_models.GATWrap
    in_channels: 11
    hidden_channels: 64
    num_layers: 6
    out_channels: 128
    act: ${load:torch.nn.functional.silu}
  decoder:
    _target_: transformers.GPT2Model
    config:
      _target_: transformers.GPT2Config
      n_positions: 32
      n_embd: 128
      n_layer: 12
      n_head: 4
      resid_pdrop: 0.0
      embd_pdrop: 0.0
      attn_pdrop: 0.0
      use_cache: false
  _target_: pl_modules.modules.in_context.InContextWrap
  graph_pooling_fn: ${load:torch_geometric.nn.global_mean_pool}
  label_emb:
    _target_: torch.nn.Linear
    in_features: 1
    out_features: 128
  label_readout:
    _target_: torch.nn.Linear
    in_features: 128
    out_features: 1
datamodule:
  _target_: pl_modules.data.datamodule.QMMineContextDataModule
  data_path: ${oc.env:PROJECT_ROOT}\data\QM9
  split_name: sizes
  batch_size: 64
  modification: null
  label_scaler:
    _target_: sklearn.preprocessing.StandardScaler
run:
  hydra:
    dir: ${oc.env:PROJECT_ROOT}/hydra/in-context/${now:%Y-%m-%d}/${now:%H-%M-%S}
trainer:
  max_epochs: 500
  log_interval: 100
